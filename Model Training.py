# -*- coding: utf-8 -*-
"""Covid_19 ML

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dxaPuGLR7IT87IEljGubspjabxuTp26D

# 2.1 Splitting dataset (10 marks)
Split the dataset into training dataset and testing dataset. Train to test ratio should be 80:20.
# 2.2 Build models (40 marks)
As mentioned above, each individual in a group has to build a classification model. The models can
be of any type, and you can use any existing Python (or R) libraries (ex: Scikit-Learn) to build them.
One out of the two/three models MUST be a variant of the boosting tree (ex: XGBoost, AdaBoost,
LightGBM). The other one/two models could include SVMs, KNN, Decision Trees, Random Forests,
MLPs, Naive Bayes or any other classifiers that you think could be appropriate for the problem
statement.
Save each trained model to your disk (if you choose .pkl file, the models would look like
xgb_classifier.pkl, rf_classifier.pkl), and include the models in submission.
# 2.3 Evaluation (30 marks)
Load the saved models from task 2.2. Once loaded, evaluate the models on both the training set and
the test set. Choose appropriate metrics to do the evaluation. Report the scores of your metrics for
the training and the test set and interpret them. Which of the metric(s) are most important for this
classification problem?
# 2.4 Overfitting (20 marks)
It is not uncommon for the classification models to overfit. Do you observe overfitting in the models
that you trained? How do you check for overfitting? Explain steps taken by using plots and/or metrics 
evaluated. You can vary the values of at least one hyperparameter, train models for different values
of that hyperparameter and then compare the performance metric on training and test dataset.

# `Please Start running the code from here`
"""

# !curl https://raw.githubusercontent.com/aamritpa/CMPT-Data-Covid-19/master/finalDataPart1.csv --output ./finalDataPart1.csv
# !curl https://raw.githubusercontent.com/aamritpa/CMPT-Data-Covid-19/master/finalDataPart2.csv --output ./finalDataPart2.csv

# Importing files
import pandas as pd
import numpy as np
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import pickle


def XGB(X, y, folds):
    # doing 80:20 split for train and test
    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=123)

    print("\nTraining the Model !")

    ##  max_depth = 6
    ##  max_depth = 7
    ##  max_depth = 8
    ##  max_depth = 9
    ##  max_depth = 10

    # preparing model pipeline
    model = make_pipeline(
        MinMaxScaler(),
        XGBClassifier(max_depth=10)
    )

    # Training the model
    model.fit(X_train, y_train)
    print("\nModel Trained succesfully !\n")

    # Save the Model
    print("\n\nSaving the Model !\n")
    target = "XGB.pkl"
    pickle.dump(model, open(target, "wb"))
    print("Model Succsesfully saved as ", target, "!")

    # Load the model
    print("\nLoading the saved Model !\n")
    source = "XGB.pkl"
    Clf = pickle.load(open(source, "rb"))
    print("\nModel Succsesfully loaded from ", target, "!\n")

    # Clf score on train data
    trainScore = Clf.score(X_train, y_train)

    # Clf score on test data
    validationScore = Clf.score(X_valid, y_valid)

    # Clf predict and predict_proba on test data
    predictY = Clf.predict(X_valid)
    predictedProbability = Clf.predict_proba(X_valid)

    # lastly cross validation score on n-folds
    if folds != 0:
        CrossValidationMean = np.mean(cross_val_score(Clf, X, y, cv=folds))
    else:
        CrossValidationMean = "Null for 0 folds !"

    # returning y valid for roc score and confusion matrix
    return y_valid, trainScore, validationScore, predictY, predictedProbability, CrossValidationMean


def naive_bayes(X, y, folds):
    # doing 80:20 split for train and test
    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=123)

    print("\nTraining the Model !")
    # preparing model pipeline
    model = make_pipeline(
        MinMaxScaler(),
        GaussianNB()
    )

    # Training the model
    model.fit(X_train, y_train)
    print("\nModel Trained succesfully !\n")

    # Save the Model
    print("\nSaving the Model !\n")
    target = "NB.pkl"
    pickle.dump(model, open(target, "wb"))
    print("Model Succsesfully saved as ", target, "!")

    # Load the model
    print("\nLoading the saved Model !")
    source = "NB.pkl"
    Clf = pickle.load(open(source, "rb"))
    print("\nModel Succsesfully loaded from ", target, "!\n")

    # Clf score on train data
    trainScore = Clf.score(X_train, y_train)

    # Clf score on test data
    validationScore = Clf.score(X_valid, y_valid)

    # Clf predict and predict_proba on test data
    predictY = Clf.predict(X_valid)
    predictedProbability = Clf.predict_proba(X_valid)

    # lastly cross validation score on n-folds
    if folds != 0:
        CrossValidationMean = np.mean(cross_val_score(Clf, X, y, cv=folds))
    else:
        CrossValidationMean = "Null for 0 folds !"
    # returning y valid for roc score and confusion matrix
    return y_valid, trainScore, validationScore, predictY, predictedProbability, CrossValidationMean


def random_forest(X, y, folds):
    # doing 80:20 split for train and test
    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=123)

    ## n_estimators=100, max_depth=12
    ## n_estimators=200, max_depth=12
    ## n_estimators=300, max_depth=12
    ## n_estimators=400, max_depth=12
    ## n_estimators=500, max_depth=12

    print("\nTraining the Model !")
    # preparing model pipeline

    model = make_pipeline(
        MinMaxScaler(),
        RandomForestClassifier(n_estimators=100, max_depth=12)
    )


    # Training the model
    model.fit(X_train, y_train)
    print("\nModel Trained succesfully !\n")

    # Save the Model
    print("\nSaving the Model !\n")
    target = "RandomForestModel.pkl"
    pickle.dump(model, open(target, "wb"))
    print("Model Succsesfully saved as ", target, "!")

    # Load the model
    print("\nLoading the saved Model !\n")
    source = "RandomForestModel.pkl"
    Clf = pickle.load(open(source, "rb"))
    print("Model Succsesfully loaded from ", target, "!\n")

    # Clf score on train data
    trainScore = Clf.score(X_train, y_train)

    # Clf score on test data
    validationScore = Clf.score(X_valid, y_valid)

    # Clf predict and predict_proba on test data
    predictY = Clf.predict(X_valid)
    predictedProbability = Clf.predict_proba(X_valid)

    # lastly cross validation score on n-folds
    if folds != 0:
        CrossValidationMean = np.mean(cross_val_score(Clf, X, y, cv=folds))
    else:
        CrossValidationMean = "Null for 0 folds !"
    # returning y valid for roc score and confusion matrix
    return y_valid, trainScore, validationScore, predictY, predictedProbability, CrossValidationMean


def plotConfusionMatrix(conf_mat, model):
    fig, ax = plt.subplots(figsize=(6, 6))
    ax = sns.heatmap(conf_mat, annot=True, cbar=False)
    plt.title('Confusion matrix', fontsize=18, fontweight='bold')
    plt.xlabel('Predicted label', fontsize=16, fontweight='bold').set_color('black')
    plt.ylabel('True label', fontsize=16, fontweight='bold').set_color('black')
    plt.show()


def CallModel(X, y, model, folds):
    y_valid, trainScore, validationScore, predictY, predictedProbability, CrossValidationMean = model(X, y, folds)
    print(f"Train Accuracy       : {trainScore * 100:.2f}%")
    print(f"Test Accuracy        : {validationScore * 100:.2f}%")
    if folds == 0:
        print(f"{folds}-fold Accuracy     :", CrossValidationMean)
    else:
        print(f"{folds}-fold Accuracy     : {CrossValidationMean * 100:.2f}%", "\n\n")

    matrix = confusion_matrix(y_valid, predictY)
    print(matrix)
    plotConfusionMatrix(matrix, "model")
    print(pd.DataFrame(classification_report(predictY, y_valid, output_dict=True)))


def to_timestamp(d):
    return d.timestamp()


import datetime


def makeDatetime(data):
    return datetime.datetime.strptime(data, '%Y-%m-%d')


def DataImportAndProcessing():
    # Importing Data
    data1 = pd.read_csv('finalDataPart1.csv')
    data2 = pd.read_csv('finalDataPart2.csv')

    # Appending IF NEEDED
    data = data1.append(data2)
    data = data.reset_index(drop=True)
    data = data.drop(columns=['index'])
    data['date_confirmation'] = data['date_confirmation'].apply(makeDatetime)
    data['date_confirmation'] = data['date_confirmation'].apply(to_timestamp)
    # Encoding Label i.e outcome column
    data['outcome'].replace({'nonhospitalized': 0, 'recovered': 1, 'hospitalized': 2, 'deceased': 3}, inplace=True)

    # Introducing dummies into data to get rid of all the categorical data
    encodedCountry = pd.get_dummies(data['country'])
    encodedProvince = pd.get_dummies(data['province'])
    encodedSex = pd.get_dummies(data['sex'])
    data = pd.concat([data, encodedSex, encodedProvince, encodedCountry], axis=1)

    print("length of filled data: ", len(data))

    data = data.drop(columns=['province', 'country', 'source', 'additional_information', 'sex'])

    return data


def main():
    data = DataImportAndProcessing()
    X = data.drop(columns=['outcome']).to_numpy()
    y = data['outcome'].to_numpy()

    print('\n\n--------------------------------------------------------------------\n\n')
    print('Calling Gaussian Naive Bayes\n')
    CallModel(X, y, naive_bayes, 0)

    print('\n\n--------------------------------------------------------------------\n\n')
    print('Calling Random Forest\n')
    CallModel(X, y, random_forest, 0)

    print('\n\n--------------------------------------------------------------------\n\n')
    print('Calling XGB Classifier\n')
    CallModel(X, y, XGB, 0)


main()
